#!/usr/bin/env python
# coding: utf-8

"""
LA Dodgers xwOBA Data
This script downloads xwOBA data from Baseball Savant for all current Dodgers players.
"""

import os
import sys
import requests
import pandas as pd
from bs4 import BeautifulSoup
import json
import time
import boto3
import logging
from io import StringIO
from datetime import datetime
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Get current year dynamically
CURRENT_YEAR = datetime.now().year

# Configuration
output_dir = "data/batting"
csv_file = f"{output_dir}/dodgers_xwoba_current.csv"
json_file = f"{output_dir}/dodgers_xwoba_current.json"
parquet_file = f"{output_dir}/dodgers_xwoba_current.parquet"
s3_bucket = "stilesdata.com"
s3_key_csv = "dodgers/data/batting/dodgers_xwoba_current.csv"
s3_key_json = "dodgers/data/batting/dodgers_xwoba_current.json"
s3_key_parquet = "dodgers/data/batting/dodgers_xwoba_current.parquet"

# List of known pitchers to skip
PITCHERS_TO_SKIP = [
    'Sasaki, Roki',
    'Snell, Blake',
    'Treinen, Blake',
    'Casparius, Ben',
    'May, Dustin',
    'Sauer, Matt',
    'García, Luis',
    'Glasnow, Tyler',
    'Wrobleski, Justin',
    'Banda, Anthony',
    'Yates, Kirby',
    'Knack, Landon',
    'Vesia, Alex',
    'Yamamoto, Yoshinobu',
    'Scott, Tanner',
    'Dreyer, Jack',
    'Phillips, Evan',
    'Kershaw, Clayton',
    'Stratton, Chris',
    'Feduccia, Hunter',
    'Trivino, Lou',
    'Kopech, Michael',
    'Miller, Bobby',
    'Davis, Noah',
    'Gonsolin, Tony'
    'Díaz, Alexis'
]

# AWS session and S3 resource
# Determine if running in a GitHub Actions environment
is_github_actions = os.getenv('GITHUB_ACTIONS') == 'true'

# AWS credentials and session initialization
aws_key_id = os.environ.get("AWS_ACCESS_KEY_ID")
aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY")
aws_region = "us-west-1"

# Conditional AWS session creation based on the environment
if is_github_actions:
    # In GitHub Actions, use environment variables directly
    session = boto3.Session(
        aws_access_key_id=aws_key_id,
        aws_secret_access_key=aws_secret_key,
        region_name=aws_region
    )
else:
    # Locally, use a specific profile
    session = boto3.Session(profile_name="haekeo", region_name=aws_region)

s3 = session.resource('s3')

headers = {
    'sec-ch-ua-platform': '"macOS"',
    'Referer': 'https://baseballsavant.mlb.com/savant-player/shohei-ohtani-660271?stats=career-r-hitting-mlb',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Not A(Brand";v="8", "Chromium";v="132", "Google Chrome";v="132"',
    'sec-ch-ua-mobile': '?0',
}

def format_player_name(name):
    """Convert 'Lastname, Firstname' to 'Firstname Lastname'."""
    if ',' in name:
        last, first = name.split(',')
        return f"{first.strip()} {last.strip()}"
    return name

def fetch_player_ids():
    """
    Scrape the Dodgers roster page to get all player IDs.
    Uses the current year dynamically to ensure we're getting the current roster.
    """
    logging.info(f"Fetching player IDs from roster page for {CURRENT_YEAR} season.")
    team_url = f'https://baseballsavant.mlb.com/team/119?view=statcast&nav=hitting&season={CURRENT_YEAR}'
    logging.info(f"Making request to: {team_url}")
    
    try:
        response = requests.get(team_url, headers=headers)
        logging.info(f"Response status code: {response.status_code}")
        
        if response.status_code != 200:
            logging.error(f"Failed to fetch roster page. Status code: {response.status_code}")
            logging.error(f"Response content: {response.text[:500]}")  # First 500 chars of response
            return {}
            
        logging.info("Successfully fetched roster page")
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all player rows in the roster table
        player_rows = soup.find_all('tr', id=lambda x: x and x.startswith('scg_'))
        logging.info(f"Found {len(player_rows)} player rows")
        
        # Create dictionary mapping player names to IDs
        player_lookup = {}
        for row in player_rows:
            try:
                player_id = row['id'].replace('scg_', '')
                # Skip team totals and MLB totals rows
                if player_id in ['119', '999999']:
                    continue
                player_name = row.find('a').text.strip()
                # Skip known pitchers before name formatting
                if player_name in PITCHERS_TO_SKIP:
                    logging.debug(f"Skipping known pitcher: {player_name}")
                    continue
                # Format name after pitcher check
                formatted_name = format_player_name(player_name)
                player_lookup[formatted_name] = player_id
                logging.debug(f"Added player: {formatted_name} (ID: {player_id})")
            except Exception as e:
                logging.warning(f"Skipping row with ID {row.get('id', 'unknown')}: {str(e)}")
                continue
        
        logging.info(f"Successfully created lookup for {len(player_lookup)} players")
        return player_lookup
        
    except Exception as e:
        logging.error(f"Error in fetch_player_ids: {str(e)}")
        logging.error(f"Response object: {response if 'response' in locals() else 'No response object'}")
        return {}

def fetch_player_xwoba(player_name, player_id):
    """Fetch xwOBA data for a specific player."""
    logging.info(f"Fetching xwOBA data for {player_name} (ID: {player_id})...")
    
    params = {
        'playerId': player_id,
        'playerType': 'Y',
    }
    
    try:
        response = requests.get('https://baseballsavant.mlb.com/player-services/rolling-thumb', 
                              params=params, 
                              headers=headers)
        logging.info(f"Response status code for {player_name}: {response.status_code}")
        
        if response.status_code != 200:
            logging.error(f"Failed to fetch data for {player_name}. Status code: {response.status_code}")
            logging.error(f"Response content: {response.text[:500]}")
            return None
            
        player_data = response.json()
        if 'plate100' not in player_data:
            logging.warning(f"No plate100 data found for {player_name} - likely a pitcher")
            return None
            
        player_data_list = player_data['plate100']
        if not player_data_list:
            logging.warning(f"No data in plate100 for {player_name}")
            return None
            
        player_df = pd.DataFrame(player_data_list)
        
        # Check if xwoba column exists
        if 'xwoba' not in player_df.columns:
            logging.warning(f"No xwoba data for {player_name}")
            return None
            
        # Convert max_game_date from UTC to Pacific Time
        if 'max_game_date' in player_df.columns:
            player_df['max_game_date'] = pd.to_datetime(player_df['max_game_date'])
            player_df['max_game_date'] = player_df['max_game_date'].dt.tz_convert('America/Los_Angeles')
            # Format the date for better readability
            player_df['max_game_date'] = player_df['max_game_date'].dt.strftime('%Y-%m-%d %H:%M:%S %Z')
            
        player_df['xwoba'] = player_df['xwoba'].astype(float)
        player_df['player_name'] = player_name
        player_df['player_id'] = player_id
        
        logging.info(f"Successfully processed data for {player_name}")
        return player_df
        
    except Exception as e:
        logging.error(f"Error fetching data for {player_name}: {str(e)}")
        logging.error(f"Response object: {response if 'response' in locals() else 'No response object'}")
        return None

def fetch_league_average_xwoba(year=None):
    """
    Fetches the league average xwOBA from the rolling leaderboard on Baseball Savant.
    It finds the inline script data, parses it, and averages the xwOBA for all batters.
    """
    logging.info("Fetching league average xwOBA from rolling leaderboard.")
    url = 'https://baseballsavant.mlb.com/leaderboard/rolling'
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching URL: {e}")
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    script_tags = soup.find_all('script')
    
    data = None
    for script in script_tags:
        if script.string and 'var rolling =' in script.string:
            script_content = script.string
            match = re.search(r'var rolling = (\{.*?\});', script_content, re.DOTALL)
            if match:
                json_data_str = match.group(1)
                try:
                    data = json.loads(json_data_str)
                    break 
                except json.JSONDecodeError:
                    continue 

    if data is None:
        logging.error("Could not find and parse rolling data from any script tag.")
        return None

    if 'Batter100' not in data:
        logging.error("Could not find 'Batter100' key in the data.")
        return None

    player_data = data['Batter100']

    if not player_data:
        logging.warning("No player data found under 'Batter100'.")
        return None

    df = pd.DataFrame(player_data)
    xwoba_column = 'last_x_xwoba'

    if xwoba_column not in df.columns:
        logging.error(f"Could not find '{xwoba_column}' column in the player data.")
        return None

    df[xwoba_column] = pd.to_numeric(df[xwoba_column], errors='coerce')
    df.dropna(subset=[xwoba_column], inplace=True)

    average_xwoba = df[xwoba_column].mean()
    logging.info(f"Calculated league average xwOBA: {average_xwoba:.3f}")
    
    return average_xwoba

def main():
    try:
        logging.info(f"Starting xwOBA data collection for {CURRENT_YEAR} season")
        os.makedirs(output_dir, exist_ok=True)
        logging.info("Output directory checked/created.")

        # Get league average xwOBA
        lg_avg_xwoba = fetch_league_average_xwoba()
        if lg_avg_xwoba is None:
            logging.error("Failed to fetch league average xwOBA. Exiting.")
            sys.exit(1)

        # Get all player IDs
        player_lookup = fetch_player_ids()
        if not player_lookup:
            logging.error("No players found in lookup. Exiting.")
            sys.exit(1)
            
        logging.info(f"Player lookup contains {len(player_lookup)} players")
        
        # Save the player lookup
        with open(f'{output_dir}/player_lookup.json', 'w') as f:
            json.dump(player_lookup, f, indent=2)
        logging.info("Saved player lookup to JSON file")
        
        # Fetch xwOBA data for each player
        all_player_data = []
        for player_name, player_id in player_lookup.items():
            player_df = fetch_player_xwoba(player_name, player_id)
            if player_df is not None:
                # Add league average column to each player's data
                player_df['league_avg_xwoba'] = lg_avg_xwoba
                all_player_data.append(player_df)
                logging.info(f"Added data for {player_name} to collection")
            time.sleep(1)  # Be nice to the server
        
        # Combine all player data
        if all_player_data:
            df = pd.concat(all_player_data, ignore_index=True)
            logging.info(f"Combined data for {len(all_player_data)} players")
            
            # Calculate forward rank, preserving original ordering
            # In Baseball Savant data, rn=1 is already the most recent plate appearance, 
            # and rn=50 is the oldest, so we'll keep this ordering
            df["rn_fwd"] = df["rn"]
            
            # Add a debugging log to show the range of rn values
            min_rn = df["rn"].min()
            max_rn = df["rn"].max()
            logging.info(f"Source data rn values range from {min_rn} (most recent) to {max_rn} (oldest)")
            
            # Validate ordering assumption if date information is available
            if 'max_game_date' in df.columns:
                # Get a sample player with multiple records
                sample_player = df['player_name'].value_counts().idxmax()
                sample_df = df[df['player_name'] == sample_player].sort_values('rn')
                
                logging.info(f"Validating chronological order using {sample_player}'s data:")
                for idx, row in sample_df.head(3).iterrows():
                    logging.info(f"  rn={row['rn']}, date={row.get('max_game_date', 'N/A')}")
                    
                logging.info("If dates for lower rn values are more recent, the ordering is correct")
            
            # Save league average separately
            league_avg_data = {
                'year': CURRENT_YEAR,
                'league_avg_xwoba': lg_avg_xwoba,
                'timestamp': pd.Timestamp.now().isoformat()
            }
            with open(f'{output_dir}/league_avg_xwoba.json', 'w') as f:
                json.dump(league_avg_data, f, indent=2)
            
            df.drop(columns=['savant_batter_id'], inplace=True)
            # Save to various formats
            df.to_csv(csv_file, index=False)
            df.to_json(json_file, orient="records")
            df.to_parquet(parquet_file, index=False)
            logging.info("Data written to JSON, CSV, and Parquet files.")
            
            # Upload to S3
            s3.Bucket(s3_bucket).upload_file(csv_file, s3_key_csv)
            s3.Bucket(s3_bucket).upload_file(json_file, s3_key_json)
            s3.Bucket(s3_bucket).upload_file(parquet_file, s3_key_parquet)
            s3.Bucket(s3_bucket).upload_file(
                f'{output_dir}/league_avg_xwoba.json',
                'dodgers/data/batting/league_avg_xwoba.json'
            )
            logging.info("Files successfully uploaded to S3.")
        else:
            logging.error("No data was collected from any players.")
            sys.exit(1)

    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")
        logging.error(f"Error type: {type(e).__name__}")
        import traceback
        logging.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()